<hr>
<p>title: Notes of Probability Theory and machine learning<br>date: 2018-12-14 22:05:56<br>tags: machine learning </p>
<h2 id="mathjax-true"><a href="#mathjax-true" class="headerlink" title="mathjax: true"></a>mathjax: true</h2><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>

<h1 id="distribution"><a href="#distribution" class="headerlink" title="distribution"></a>distribution</h1><h2 id="Gaussian-distribution"><a href="#Gaussian-distribution" class="headerlink" title="Gaussian distribution"></a>Gaussian distribution</h2><p>defined over a D-dimensional vector $\mathbf { x }$ of continuous variables,</p>
<script type="math/tex; mode=display">
\mathcal { N } ( \mathbf { x } | \boldsymbol { \mu } , \mathbf { \Sigma } ) = \frac { 1 } { ( 2 \pi ) ^ { D / 2 } } \frac { 1 } { | \mathbf { \Sigma } | ^ { 1 / 2 } } \exp \left\{ - \frac { 1 } { 2 } ( \mathbf { x } - \boldsymbol { \mu } ) ^ { \mathrm { T } } \mathbf { \Sigma } ^ { - 1 } ( \mathbf { x } - \boldsymbol { \mu } ) \right\}</script>